\documentclass[sigconf,review,anonymous]{acmart}
\usepackage{algorithmicx}
\usepackage{amsmath}
%\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{xspace}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
    \providecommand\BibTeX{{%
        Bib\TeX}}}

\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[ICPP '24]{53rd International Conference on Parallel Processing}{August 12-15, 2024}{Gotland, Sweden}
\acmISBN{978-1-4503-XXXX-X/18/06}

\include{macros}

\newcommand{\algo}[1]{\textsc{#1}}
\newcommand{\bottomlevel}[1]{\underline{l}_{#1}} % underline short italic
\newcommand{\criticalpath}{\mathcal{P}}
\newcommand{\parents}[1]{\,\Pi_{#1}}
\newcommand{\children}[1]{\,C_{#1}}
\newcommand{\cluster}{\,\mathcal{S}}
\newcommand{\daghetpart}{\algo{DagHetPart}\xspace}
\newcommand{\dagmem}{\algo{DagHetMem}\xspace}

\newcommand{\MM}{M}
\newcommand{\MC}{MC}
\newcommand{\rt}{rt}
\newcommand{\curM}{curM}
\newcommand{\curC}{curC}
\newcommand{\PD}{PD}

\newcommand{\skug}[1]{{\color{blue}[SK: #1]}}
\newcommand{\hmey}[1]{{\color{red}[HM: #1]}}
\newcommand{\AB}[1]{{\color{purple}[AB: #1]}}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
    \title{Adaptive Scheduling of Scientific Workflows}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
    \author{Svetlana Kulagina}
    \email{svetlana.kulagina@hu-berlin.de}
    \orcid{0000-0002-2108-9425}
    \affiliation{%
        \institution{Humboldt Universitaet zu Berlin}
        \streetaddress{Unter den Linden 6}
        \city{Berlin}
        \country{Germany}
        \postcode{10099}
    }

       \author{Henning Meyerhenke}
       \affiliation{%
           \institution{Humboldt Universitaet zu Berlin}
           \city{Berlin}
           \country{Germany}
       }
       \email{meyerhenke@hu-berlin.de}

    \author{Anne Benoit}
    \affiliation{%
        \institution{ENS Lyon}
        \city{Lyon}
        \country{France}
    }
    \email{Anne.Benoit@ens-lyon.fr}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
    \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
    \begin{abstract}
       Todo: reinsert CCSXML concepts
    \end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
    \keywords{Scheduling, Adaptive Scheduling, DAG}
    \received{15 April 2024}
    \received[revised]{12 March 2009}
    \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
    \maketitle

    \section{Introduction}


    \section{Model}

    \begin{table}[h]
        \begin{center}
            \begin{tabular}{rl}
                \hline
                \textbf{Symbol} & \textbf{Meaning}  \\
                \hline
                $G = (V, E)$  & Workflow graph, set of vertices (tasks) and edges  \\
                $\parents{u}$, $\children{u}$ & Parents of a task $u$, children of a task $u$ \\
                $m_u$& Memory weight of task $u$ \\
                $w_u$   & Workload of  task $u$  (normalized execution time)    \\
                $c_{u,v}$   & Communication volume along the edge $(u,v)\in E$ \\
                $F$, $\mathcal{F}$ & A partitioning function and the partition it creates \\
                $V_i$ & Block number $i$\\ %\wrt~some $F$   \\
                $\cluster$, $k$   & Computing system and its number of processors   \\
                $p_j$, proc($V_i$)  & Processor number $j$, processor of block $V_i$ \\
                $M_j$, $s_j$   & Memory size and speed of processor $p_j$   \\
                $\beta$ & Bandwidth in the compute system  \\
                $\bottomlevel{u}$   & Bottom weight of task $u$   \\
                $\mu_G$, $\mu_i$ & Makespan of the entire workflow $G$ and of a block $V_i$ \\
                $\Gamma = (\mathcal{V}, \mathcal{E})$ & Quotient graph, its vertices and its edges  \\
                $r_u$, $r_{V_i}$  & Memory requirement of  task $u$ and of block $V_i$   \\
                $r_{\max}$   & Maximum memory requirement in a workflow   \\
                $\criticalpath$ & Critical path in a workflow  \\
                \hline
            \end{tabular}
        \end{center}
        \caption{Notation.} \label{tabnotation}
    \end{table}

    \paragraph{Workflow-related changes}

    \begin{itemize}
        \item A task $v$ takes longer or shorter to execute than planned: its time weight $w_u$ changes to $w'_u$.
        \item A task $v$ takes more or less memory to execute than planned: its memory requirement $m_v$ changes to $m'_v$.

    \end{itemize}

    The following changes are not a part of this article's scope:

    \begin{itemize}
        \item The workflow structure changes: edges or tasks come in or leave.
    \end{itemize}

    \paragraph{Execution environment-related changes }


    \begin{itemize}
        \item A processor exists the execution environment: $k$ decreases and $\cluster$ changes.
        \item A processor enters the execution environment: $k$ increases, $\cluster$ gets a new processor with possibly new memory requirement and processor speed.

    \end{itemize}

    The following changes are not a part of this article's scope:

    \begin{itemize}
        \item Processor characteristics change: the memory requirement or speed become bigger or smaller
    \end{itemize}

    \subsection{Time of changes }

    We consider discrete time in seconds.
    The time point(s) at which the changes happen is unambiguously defined.

    For any task $v$, its runtime equals its time weight divided by the speed of the processor $p_j$ it has been assigned to: $w_v/s_j$.
    The start time of any task $v$ is its top level($\bar{l}_v$), or the difference between the maximum bottom level in the workflow (the makespan of the workflow) and the task's own bottom level: $\bar{l}_v = \mu_\Gamma - \bottomlevel{v}$.
    The start time of the source task in the workflow is zero.
    The end time of a task $v$ is its start time and its runtime: $\bar{l}_v + w_v/s_j$

    \subsection{Changes and knowledge horizon - important questions TBA}

    Given a valid mapping of tasks to processors, we can say what we predicted would happen at any given time point $T$: what tasks have been executed, what have not finished or have not even started.

    At the point of change, we know that some tasks that finished took longer than expected ($w_v$ are bigger) or shorter.
    However, how do we model the following:
    \begin{itemize}
        \item Do we know the new weights of currently running tasks and tasks that have not yet started? This means, do we foresee into the future or do we assume that all weights on unfinished tasks remain the same?
        \item A change in memory requirements can mean that the assignment had been invalid. Do we assume that these tasks failed and we need to rerun them?
        \item How many times of change do we model - one per workflow run, or multiple?
        \item At what time does the change and reevaluation happen - is it a fixed (random?) point of time or is it workflow-dependent (say, after 10\% of the workflow is ready)?
    \end{itemize}

    \section{Related work}

    In the limited time I spent looking, I found no paper addressing the exact same problem.

    Wang et al.~\cite{wang2019dynamic} proposes a dynamic particle swarm optimization algorithm to schedule workflows in a cloud.
    Particles are possible solution in the solution space.
    However, the dynamic is only in the choice of generation sizes, not in the changes in the execution environment.
    Singh et al.~\cite{singh2018novel} addresses dynamic provisioning of resources with a constraint deadline.
    However, the approach is for clouds.
    \xspace

    Daniels et al.~\cite{daniels1995robust} formalize the concept of robust scheduling with variable processing times
    on a single machine.
    The changes in runtimes of tasks are not due to changing machine properties, but are rather task-related (that means
    that these runtime changes are unrelated to each other).
    The authors formulate a decision space of all permutations of n jobs, and the optimal schedule in relation to a
    performance measure $\phi$.
    Then they proceed to formulate the Absolute Deviation Robust Scheduling Problem as a set of linear constraints.

    De Olivera~\etal~\cite{de2012provenance} propose a tri-criteria (makespan, reliability, cost) adaptive scheduling heuristic
    for clouds.
    Based on a 3-objective cost model, their greedy scheduling algorithm schedules tasks into machines.
    The authors use provenance data to make scheduling decisions.
    The cost model is a set of linear equations computed in the simulation environment, it represents the cost of an execution based on the criteria.
    In the algorithms, the authors test out 4 scenarios - one preferring each criteria, and a balanced one.
    The algorithm merely chooses the best virtual machine for each next task based on the cost given by the model.
    An additional algorithm combines several cloud activities (task executions) into one to improve the cost, so that each
    execution is not too small (utilize the granularity factor, the smallest entity of payment of the cloud producer).
    The authors used workflows with less than 10 tasks, but repeated them so that the execution had up to 200 tasks.
    They do not report the runtime of the scheduling algorithm, only the speedup and cost saving it produces.

    Rahman~\etal~\cite{rahman2013} propose a scheduling heuristic for grids that proposes mapping of tasks to machines by calculating
    the critical path in the graph dynamically at every step.
    They call it the dynamic critical path (DCP).
    For all tasks they compute the earliest start time and absolute latest start time that are upper and lower bounds
    on the start time of a task (differing by the slack this task has).
    All tasks on this critical path have the same earliest and latest start times, because they cannot be ddelayed.

    The algorithm takes the first unscheduled task on the critical path each time and maps it on a processor identified for it.
    If processors are heterogeneous, then the start times are computed with respect for the processor, and the minimum execution time for
    the task is chosen.
    The heuristic also uses the same processor to schedule parent and children tasks, as to avoid data transfer between processors.
    The authors evaluate their approaches on random workflows of the size up to 300 tasks.

    The authors provide an overview over (simpler) scheduling heuristics.
    For example, GRASP (generally randomized adaptive search procedure) conducts a number of iterations to search an optimal
    solution for mapping tasks on machines.
    A solution is generated at each step, and the best solution is kept at the end.
    The search terminates when a certain termination criterion is reached.
    It generates better results than other algorithms, because it explores the whole solution space.

    Avanes~\etal\cite{avanes2008adaptive} present a heuristic for networks in disaster scenarios.
    These networks are a set of DAG-shaped scenarios, out of which one needs to be executed.
    The scenario contains AND- and OR-branches, where AND-branches indicate acitivities that need to be executed in parallel.

    The heuristic first partitions the set into local schedules by using affinity matrices to determine similar
    activities and group them together.
    Then they physically allocate these partitions to groups of disaster responders and tasks within this group.
    They define a constraint system for that.
    The dynamic part deals with changes and distinguishes between retriable and compensation acitivities.
    The heuristic calculates a new execution path with these tasks.
    In general, this heuristic is not as related as it looks, because of very specific workflow and task structure.

    Garg~\etal~\cite{GARG2015256} propose a dynamic scheduling algorithm for heterogeneous grids based on rescheduling.
    The aim is to minimize the makespan, and the experiments were conducted on a single wotkflow with 10 tasks.
    The procesdure involves building a first (static) schedule, priodic resource monitoring and rescheduling the remaining
    tasks.
    The resource model contains resource groups (small tightly-connected sub-clusters), connected between each other.
    For each resuorce group, there is an own scheduler, and an overall global scheduler responsible for distributing
    tasks to groups.
    The authors define the execution time, estimated start time, data ready time,a dn estimated finish time per task.
    The runtimes of tasks depend on processor speeds, are calculated in advance and stored in tables.

    The algorithm first computes bottom levels for all tasks (execution time is average of all possible execution times).
    THe bottom level represents the priority of the task, and tasks are sorted according to these priorities.
    They then go through tasks and map than to such processors that minimize the earliest start times of this task's
    successors.
    To do this, the authors calculate the earliest finishing time of the task across all ressources, along with the
    average communication and computation costs fir the dependent tasks.

    The rescheduling is being triggered when either a load on a resource increases over a threshold, or if a new resource
    is added.
    The algorithm produces a new mapping from scrath, and this mapping is being accepted if the resulting maespan is
    smaller than the previously predicted one.

\section{Proposal of a new heuristic with slightly refined model}

The idea is to get rid of the constraint that a processor handles a {\em block} of tasks,
but favor processor reuse as is done in HEFT. 
Furthermore, this would allow us to handle variability on the fly, by updating
the bottom levels if some parameters vary, and computing the schedule
only for the near future...

\subsection{Model}

For a task~$u\in V$, we have a memory usage~$m_u$ and execution time~$w_u$.
For an  edge $(u,v)\in E$, we have a data of size $c_{u,v}$. 

For each processor $P_j$, we have a speed~$s_j$, and also two memory bounds: 
$\MM_j$ the processor memory, and $\MC_j$, the size of the communication buffer.
We can decide to evict some data from the main memory if we are sending the data
to another processor; it then stays in the communication buffer until it has been sent.

We keep track of the current ready time of each processor and each communication
channel, $\rt_j$ and $\rt_{j,j'}$, for all processors~$(j,j')$. Initially, all the ready times
are set to~$0$. 

We also keep track of the currently available memory, $availM_j$ and $availC_j$,
on respectively the processor memory and communication buffer. 
Furthermore, $\PD_j$ is a priority queue with the {\em pending data} 
that are in the memory of size $\MM_j$ but may be evicted to be communicated, if 
more memory is needed on~$p_j$. They are ordered by non-decreasing size. 
They correspond to some $c_{u,v}$'s.

\subsection{Baseline: original HEFT without memories}

    Original HEFT does not consider memory sizes.
    The solutions it provides can be invalid if it schedules tasks to processors without sufficient memories.
    However, these solutions can be viewed as a ``lower bound'' for an actual solution that considers memory sizes.

    HEFT works in two steps.
    In the first step, it calculates the ranks of tasks by computing their non-increasing bottom levels.
    The bottom level of a task is defined as
    $$bl(u) = w_u + \max_{(u,v)\in E} \{c_{u,v} + bl(v)\}$$
    (the max is 0 if there is no outgoing edge).
    The tasks are sorted by non-decreasing ranks.

    In the second step, the algorithm iterates over the ranks and tries to assign the task to the processor where it
    has the earliest finish time.
    We tentatively assign each task to each processor.
    The task's starting time $st_v$ is dictated by the maximum between $rt_j$, and all communications that
    must be orchestrated from predecessor tasks $u\notin T(p_j)$.
    The starting time is then
    \[ST(v, p_j) = \max{ \{rt_j, \max_{ u \in \Pi(v)}\{ FT(u)+ c_{u,v} / \beta , rt_{proc(u), p_j} + c_{u,v} / \beta  \} \} } \]
    Its finish time on $p_j$ is then
    $FT(v,p_j) = st_v + \frac{w_v}{s_j}$.

    Once we have computed all finish times for task~$v$,
    we keep the minimum $FT(v,p_j)$ and assign task~$v$
    to processor~$p_j$.

    \textit{Assignment to processor}
    When assigning the task, we set the ready time of the processor~$j$ $rt_j$ to the finish time of the task.
    For every predecessor of~$v$ that has been assigned to another processor, we adjust ready times on
    communication buffers $rt_{j', j}$ for every predecessor $u$'s processor $j'$: we increase them by the
    communication time $c( u,v) / \beta$.


\subsection{Heuristics}
    Like the original HEFT, our heuristic consistst of two steps: first, computing task ranks,
    and second, assigning tasks to processors in the order defined in the first step.
    We consider three variants of HEFT accounting for memory usage, which only
    differ in the order they consider tasks to be scheduled.

\subsubsection{Step 1: calculate task ranks}

HEFTM-BL orders tasks by non-increasing bottom levels, where the bottom
level is defined as
$$bl(u) = w_u + \max_{(u,v)\in E} \{c_{u,v} + bl(v)\}$$
 (the max is 0 if there is no outgoing edge). 
 
 HEFTM-BLC: from the study of the fork (see below), it seems important
 to also account for the size of the data as input of a task,
 to give more priority at tasks with potential large incoming communications.
 For each task, we compute its modified bottom level: 
 $$blc(u) = w_u + \max_{(u,w)\in E} \{c_{u,w} + blc(w)\} + \max_{(v,u)\in E} c_{v,u}   . $$
 
    \skug{avoid having mixed ranks, when the memory size of the lower task is not taken into account}
 
HEFTM-MM orders tasks as dictated by MinMem.

\subsubsection{Task assignment}

Then, the idea is to pick the next free task in the given order,
and greedily assign it to a processor, by trying all possible options
and keeping the most promising one. 

\medskip
\noindent{\em Tentative assignment of task~$v$ on $p_j$.}\\
{\bf Step 1.} First, we need to check that for all predecessors~$u$ of~$v$ that are mapped
on~$p_j$, the data $c_{u,v}$ is still in the memory of~$p_j$, 
i.e., $c_{u,v}\in PD_j$. Otherwise, the finish time is set to~$+\infty$ (invalid choice). 

\smallskip
\noindent{\bf Step 2.} Next, we check the memory constraint on~$p_j$, by computing
\[Res = availM_j  - m_v - \sum_{u \in \Pi(v), u\notin T(p_j)}  \{c_{u,v}\}
- \sum_{w\in Succ(v)}  \{c_{v,w}\}.\]

$T(p_j)$ is the set of tasks already scheduled on $p_j$; by step 1, their files are
already in the memory of~$p_j$. However, the files from the 
other predecessor tasks must be loaded in memory before executing task~$v$,
as well as $m_v$ and the data generated for all successor tasks.
$Res$ is then checking whether there was enough memory; if it is negative,
it means that we have exceeded the memory of~$p_j$ with this tentative 
assignment. 

In this case ($Res <0$), we  try evicting
some data from memory so that we have enough memory to execute task~$v$. 
We need to evict at least $Res$ data.
For now, we propose a greedy approach, evicting the smallest files of $\PD_j$ until $Res$ data has been evicted,
in order to avoid costly communications. 
\AB{FYI We initially discussed  evicting the largest files, but this leads to 
large communications and does not seem efficient after all... Maybe we can think of another 
approach that would take into account both data size and bottom level...}
While tentatively evicting files, we remove them from the list of pending memories and move them into a list
    of memories pending in the communication buffer.
We keep track of the available buffer size, too - each time a file gets moved into the pending in buffer, the available buffer size is reduced by its weight.

If we still do not have enough memory after having tentatively evicted all files from $\PD_j$,
    or if while doing so we exceeded the size of the available buffer,
we set the finish time to~$+\infty$ (invalid choice). 

\smallskip
\noindent{\bf Step 3.} We tentatively assign task~$v$ on $p_j$.
    Its starting time $st_v$ is dictated by the maximum between $rt_j$, and all communications that
must be orchestrated from predecessor tasks $u\notin T(p_j)$.
    The starting time is then
    \[ST(v, p_j) = \max{ \{rt_j, \max_{ u \in \Pi(v), u\notin T(p_j)}\{ FT(u) , rt_{proc(u), p_j}\} + c_{u,v} / \beta \} } \]
    Its finish time on $p_j$ is then
$FT(v,p_j) = ST(v, p_j) + \frac{w_v}{s_j}$.



\medskip
\noindent{\em Assignment of task~$v$.}\\
Once we have computed all finish times for task~$v$, 
we keep the minimum $FT(v,p_j)$ and assign task~$v$
to processor~$p_j$.
In detail, we:
    \begin{itemize}
        \item  Evict the file memories that correspond to edge weights that need to be evicted to free the memory.
        We remove these files from pending memories
        $PD_j$, add them to pending data in the communication buffer, and reduce the available buffer size accordingly.
        \item    Calculate the new $availM_j$ on the processor.
        We subtract the weights of all incoming files from predecessors assigned to the same processor,
        and add the weights of outgoing files generated by the currently assigned task.
        \item  For every predecessor of~$v$ that has been assigned to another processor, we adjust ready times on
        communication buffers $rt_{j', j}$ for the processor~$j'$that the predecessor $u$ has been assigned to: we increase them by the
        communication time $c( u,v) / \beta$.
        We also remove the incoming files from either the pending memories or pending data in buffers of these other
        processors, and increase the available memories or available buffer sizes on these processors.
        \item We compute the correct amount of available memory for $p_j$ (for when the task is done).
        For each predecessor that is mapped to the same processor, we remove the pending memory corresponding to the weight of
        the incoming edge, also freeing the same amount of available memory (increasing $availM_j$).
        For each successor, on the other hand, we add the edge weights to pending memories and reduce $availM_j$ by the corresponding
        amount.
    \end{itemize}


\subsection{Retracing the effects of change on an existing schedule}
    After the monitoring system has reported changes, we need to assess their impact on the existing schedule.
    These changes can invalidate the schedule (\eg if there is not enough memory for some tasks to execute anymore),
    they can lead to a later finishing time (\eg if some tasks longer and delay other tasks), or they can have no effect (\eg if new processors
    joined the cluster, but the old schedule did not account for them).
    To assess the impact, we need to retrace the schedule.

    First, we find out if at least one processor that had assigned tasks has exited - this instantly invalidates the
    entire schedule.

    We then iterate over all tasks of the workflow in a topological order - any of the orderings given by rankings BL, BLC or MM
    is a topologial ordering.
    We then repeat steps similar to those we did during tentative assignment in our heuristic, except we do not choose a processor
    anymore, but rather see if the current one still fits.

    For each task $v$, we first assess its current memory constraint $Res$ using Step 2 from our heuristic.
    The factors that affect $Res$ are possible changes in $m_v$, in $c_{u,v}$ from predecessors $u$ or $c_{v,w}$ from successors $w$,
    available memory $availM_j$ on the processor (due to either changed $M_j$ or changed memory requirements from other tasks).
    If originally,$Res$ was positive (no files were evicted from memory into the communication buffer), then it has to stay this way -
    otherwise evicted files can invalidate next tasks.
    If original $Res$ was negative, then we need to make sure that evicted files still fit into the communication buffer.
    If either $Res$ is newly negative, or the communication buffer is not large enough, this invalidates the schedule.
    We update the $availM_j$ and $availMC_j$ according to the new memory constraints.

    Then we can re-calculate the finish time of the task on its processor like in Step 3.
    The factors that affect it are changes in own execution time $w_v$ of the tasks, changed ready time of the processor
    (after delayed previous tasks), and changed communication buffer availability.

    Then, after having updated the processor's values, we move on to the next task.



\subsection{The fork}
We look at the behavior of these heuristics on a fork graph,
where there is a root task~$T_0$, producing $n$ files $f_1, \ldots, f_n$
to be used by tasks $T_1, \ldots, T_n$ ($f_i = c_{0,i}$).

Without memory, this problem is NP-complete; this is equivalent
to 2-partition if the tasks have $w_i=a_i$, and all files are of size~$f_i=0$,
and with two processors. Half of the tasks must be sent to the processor
on which $T_0$ is not executed, and the optimal makespan is 
$w_0+\frac{1}{2}\sum_{1\leq i \leq n} w_i$.

However, with an infinite number of identical processors, it can be
solved in polynomial time: sort tasks by non-decreasing $f_i+w_i$; 
the $k$ tasks with smallest $f_i+w_i$ are then sent to another processor,
while the remaining $n-k$ tasks are executed locally (try all values of $k$).

With heterogeneous processors, it is probably NP-complete again
because we could ensure that there are only two processors fast enough
and get back to the 2-partition...

We also had an example where evicting large files first in step 2
can lead to arbitrarily bad makespan. Consider a fork with $n=2$,
$f_1=1$, $w_1=2$, $f_2=100$, $w_2=1$, and memory constraint
imposes that we free one unit of memory before executing one
of the tasks... Actually the new version with BLC would start 
considering $T_2$ and be fine in this case...


\AB{Can we prove that we have (maybe) a 2-approximation, 
at least for the fork? What worst-case can we think of? }




\subsection{Approximation}
\hmey{Rough notes:}
Let's use a fork to see how the algorithm behaves and if it provides some approximation. Our current intuition is that, if the memory constraint is ignored, HEFTM-BLc provides a $2$-approximation (to be proved).



\section{Conclusion}



\section{Appendix}


    \subsection{New heuristic: Local search}
    One strategy that can be used is local search, a similar approach as with swaps.
    The starting point could be a previously valid solution (for example, one by HetPart).
    In the first step, the solution would need to become valid again.
    Then, we repartition blocks on the critical path  and merge blocks in attempt to improve makespan.
    Finally, we swap blocks between processors to finish makespan improvement.

    \subsubsection{Restore Validity}
    We~(Algorithm~\ref{alg:RestoreValidity}) start with the previously partitioned and assigned quotient tree $\Gamma$,
    and the new weights on tasks and edges.
    We first recompute the memory requirements of all blocks.
    Then we identify all blocks that have become invalidated and exceed the memory of their assigned processor
    (Line~\ref{line:damaged}).
    For all these blocks, we try out ``border'' tasks, that is, tasks, that have neighbours in neighbouring blocks
    (Line~\ref{line:neighb}).
    We tentatively move them (creating a temporary quotient DAG $\Gamma'$) and assess the impact of this move on the
    makespan(Line~\ref{line:makespan}).
    The condition is, of course, that the neighboring block now does not exceed the memory of its own processor.
    The task that delivers the best makespan in then moved in Line~\ref{line:movebest}.
    The entire process is being repeated until the block fits into the memory of the processor again.
    We return the modified quotient graph $\Gamma$ that contains the mapping to processors.

    \begin{algorithm}[h!]
        \caption{Restore Validity}
        \label{alg:RestoreValidity}
        \begin{algorithmic}[1]
            \Procedure{Restore Validity}{$\Gamma$, $W$ }\\
            \Comment{Input: initial schedule as quotient tree $\Gamma$, new  weights $W$}
            \State $\Gamma \gets $ \textsc{QuotientDAG}($\Gamma$, $W$ ); \Comment Quotient DAG with new weights

            \State $D \gets \emptyset$ \Comment {Damaged blocks that require action}
            \For{$\nu_i \in \Gamma$}
                \State $r_{\nu_m} \gets W$
                \Comment{Recompute the new memory requirements of blocks in the quotient DAG}
            \EndFor
            \For{$\nu_i \in \Gamma$}
                \label{line:damaged}
                \If{$r_{\nu_i} > M_{proc(\nu_i)}$}
                    \State $D \gets D \cup \{\nu_i\}$
                \EndIf
            \EndFor

            \For{$\nu_i \in D$}
                \Comment Restore the validity of each damaged block
                \While{$r_{\nu_i} > M_{proc(\nu_i)}$}
                    \State $MS_{min}\gets \infty; v_{min} \gets NULL; \nu_{min} \gets NULL$
                    \Comment Moving $v_{min}$ to $\nu_{min}$ gives $MS_{min}$
                    \For{$v \in \nu_i$}
                        \For{$u \in ( \Pi_v \cup C_v) \cap (\Pi_{\nu_i} \cup C_{\nu_i})$}   \label{line:neighb}
                        \State $\Gamma' \gets \Gamma \mid \nu(v) == \nu(u)$
                        \If{ $r_{\nu(u)} > M_{proc(\nu(u))}$ and \textsc{Makespan}($\Gamma'$)$\leq MS_{min}$}\label{line:makespan}
                        \State $MS_{min} \gets $\textsc{Makespan}($\Gamma'$); $v_{min} \gets v$; $\nu_{min} \gets \nu(u);$
                        \EndIf
                        \EndFor
                    \EndFor
                    \State  $\Gamma \gets \Gamma \mid \nu(v_{min}) == \nu_{min}$ \label{line:movebest}
                \EndWhile
            \EndFor
            \State return $\Gamma$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm}


    \subsubsection{Repartition and Merge}

    In the next step, we center our attention around changes in the execution environments and how the changes in
    the workflow reflect on it.

    First, we try to further partition blocks that lie on the critical path of the workflow.
    We first compute the critical path (Line~\ref{line:cp}) and then, as long as there are free processors in the
    execution environment(Line~\ref{line:whilefree}, assumed to be sorted in descending order of memory sizes),
    we partition the blocks on the critical path(Line~\ref{line:repartition}).
    We try to partition each block into 2 smaller blocks.
    However, due to the nature of the partitioner, more can be created.
    We assume that the resulting blocks(Line~\ref{line:repartition}) are given back sorted by their memory requirement.
    We then assign them to free processors as long as the blocks fit and there are free processors.
    If even one part does not fit(Line~\ref{line:ifnotfits}), then we discard the entire partition and try partitioning
    the next block on the critical path.

    Not only partitioning (introducing more parallelism) can lead to a makespan improvement.
    Shorter runtimes of tasks can justify merging some blocks together.
    For all pairs of blocks that are neighbours (Line~\ref{Line:neighbblocks}), we tentatively merge them and assign
    the result to the processor of the first block.
    If the merged block fits on this processor and the merged quotient graph has a smaller overall makespan, then we save
    the merge and proceed with other blocks.

    \begin{algorithm}[h!]
        \caption{Repartition and Merge}
        \label{alg:RepartitionMerge}
        \begin{algorithmic}[1]
            \Procedure{Repartition and Merge}{$\Gamma$, $\cluster$ }\\
            \Comment{Input: quotient tree $\Gamma$, execution environment $\cluster$}
            \State $CP \gets $\textsc{Criticalpath}($\Gamma$) \label{line:cp}
            \State $Free \gets \{ p \in \cluster \mid p$ is free$\}$
            \While{$\mid Free \mid \neq 0$}\label{line:whilefree}
            \For{$\nu \in CP$}
                \State $\{ \nu_1, \nu_2,\dots\}\gets$\textsc{Partition}($\nu$,2) \label{line:repartition}
                \For{$\nu' \in \{ \nu_1, \nu_2,\dots\}$}
                    \If{$r_\nu' \leq Free.$head()}
                        \State proc($\nu'$)$\gets Free.$head();
                    \Else \label{line:ifnotfits}
                    \State \textsc{Merge}$\nu' \in \{ \nu_1, \nu_2,\dots\}$; break;
                    \Comment Merge the parts back and leave this partitioning, as it does not fit
                    \EndIf
                \EndFor
            \EndFor
            \EndWhile

            \For{$\nu_1, \nu_2 \in \Gamma \mid \nu_1 \in ( \Pi_{\nu_2} \cup C_{\nu_2})$}
                \label{line:neighbblocks}
                \State $\{\Gamma', \nu_m\} \gets $ \textsc{Merge}($\nu_1, \nu_2$); proc($\nu_m$)$\gets $ proc($\nu_1$);
                \State $\mu \gets $\textsc{Makespan}($\Gamma'$)
                \If{$r_{\nu_m} \leq M_{proc(\nu_m)}$ and $\mu \leq $ \textsc{Makespan}($\Gamma$)}
                    \State $\Gamma \gets \Gamma'$
                \EndIf
            \EndFor

            \State return $\Gamma$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm}



    \subsubsection{Swaps}

    Then a process similar to Step 4 in %\daghetpart
    can be executed: the newly rearranged blocks are swapped until no
    improvement in makespan can be achieved.

    \subsection{New heuristic: HEFT with minimal memory traversal}

    This heuristic changes both the rank calculation step, and the assignment step.

    Before calculating ranks, we calculate the optimal memory traversal over the entire graph.
    We then calculate ranks as bottom levels on each processor, but adding some small $\epsilon$ on tasks that have the
    same bottom level, but are higher in the memory traversal order.
    So, for a traversal $v_1 \rightarrow v_2 \rightarrow v_3$, if both $v_2$ and $v_3$ have the bottom level of $100$,
    $v_2$ will get the rank of $100+\epsilon$.

    Then we sort the tasks ascending according to their rank.
    We then look to minimize not the earliest finishing time of the task itself, but the earliest starting time of
    its \skug{heaviest} successor.
    The term heaviest is currently not defined exactly.
    The intuition is to try to optimize for that successor that hold the schedule up the most.

    We then do a procedure similar to our FillUpFit intuition.
    We traverse the graph along the optimal memory traversal.
    We keep track of the processor that we last used.
    With each new task $v$, we check if it has the best rank on the same processor.
    If so, and if the processor's memory is big enough, we fit the task there.

    If the memory is too full or if $v$ has a better rank elsewhere, we calculate earliest starting times
    of all its successors on all other processors (we assume that not only $v$ itself goes on that processor, but also
    all its successors).
    We then choose the processor that gives the best earliest starting time for the  \skug{heaviest} successor
    and assign $v$ there.


    \bibliographystyle{ACM-Reference-Format}
    \bibliography{references}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.